{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo para acesso e uso do Pyspark (Python com Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas (importação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import psutil\n",
    "import shutil\n",
    "import findspark\n",
    "import pathlib as path\n",
    "\n",
    "### Data\n",
    "from datetime import datetime\n",
    "\n",
    "## Data\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "### GCP\n",
    "from google.cloud import bigquery\n",
    "\n",
    "### Spark\n",
    "from pyspark               import SparkContext, SparkConf\n",
    "from pyspark.sql           import SparkSession\n",
    "\n",
    "## Suporte ao IPYTHON\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variáveis globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__SPARK = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_START       = time.time()                  ## Hora atual do sistema\n",
    "DATE_TIME        = datetime.now()               ## Data atual do sistema\n",
    "SPARK_PATH       = f'/opt/data/spark'           ## Caminho onde os logs e processos do spark serão tratados\n",
    "PROJECT_NAME     = '[NOME DO PROJETO]'          ## Utilize nessa variável o nome do projeto do GCP\n",
    "BUCKET_NAME      = '[NOME DO STORAGE]'          ## Utilize nessa variável o nome do seu storage name do GCP\n",
    "PATH_APPLICATION = os.path.abspath(os.getcwd()) ## Caminho físico de onde está a aplicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_create(folder:str):\n",
    "    \"\"\"\n",
    "    Verifica se a pasta existe localmente, e caso não exista a mesma será criada\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    folder : str\n",
    "        Caminho completo com o nome da pasta de criação caso a pasta não exista será criada\n",
    "    \"\"\"\n",
    "\n",
    "    # Verifica se o diretório destino existe, caso contrário cria o caminho\n",
    "    path.Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def folder_delete(folder:str):\n",
    "    \"\"\"\n",
    "    Verifica se o arquivos existe, e caso exista o mesmo será excluído da pasta local\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    folder : str\n",
    "        Caminho completo com o nome da pasta de criação caso a pasta não exista será excluído\n",
    "    \"\"\"\n",
    "\n",
    "    ## Verifica se o arquivo existe\n",
    "    shutil.rmtree(folder, ignore_errors=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def file_split(file:str):\n",
    "    \"\"\"\n",
    "    Fragamenta o arquivo em partes: caminho + extensão\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    file : str\n",
    "        Caminho completo com o nome do arquivo ['/home/email_statistics.sql']\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    folder : str\n",
    "        Caminho completo composto pelo nome do arquivo sem a extensão ['/home/email_statistics']\n",
    "    extension : str\n",
    "        Extensão do arquivo sem separador ['sql']\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Fragmenta o caminho e arquivo em partes\n",
    "    folder, extension = os.path.splitext(file)\n",
    "    extension = extension.replace('.', '')\n",
    "\n",
    "    ## Retorna a pasta base + nome da pasta com o nome do arquivo + nome do arquivo + extensão do arquivo\n",
    "    return folder, extension\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def file_extension(file:str):\n",
    "    \"\"\"\n",
    "    Extrai a extensão do arquivo\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    file : str\n",
    "        Caminho completo com o nome do arquivo\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    extension : str\n",
    "        Extensão do arquivo com o separador ['.sql']\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variável\n",
    "    extension = None\n",
    "\n",
    "    try:\n",
    "        ## Extrai a extensão do arquivo\n",
    "        extension = path.Path(file).suffix.lower().strip()\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "    ## Retorna o valor obtido\n",
    "    return extension\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def get_memory():\n",
    "    \"\"\"\n",
    "    Retorna a quantidade de memória total disponível do sistema (reduzindo 1g para controle)\n",
    "    ----------\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    mem_gib : string\n",
    "        Memória total disponível (formato 8g)\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variável local\n",
    "    memory = '4g'\n",
    "\n",
    "    ## Recupera a memória total em bytes\n",
    "    mem_tot = psutil.virtual_memory().total\n",
    "    ## Converte a memória total em bytes para gigabytes\n",
    "    mem_gib = round(int(mem_tot/(1024.**3)) * 0.7)\n",
    "\n",
    "    ## Verifica o tamanho mínimo e ajusta se for o caso\n",
    "    if mem_gib <= 4: mem_gib = 4\n",
    "\n",
    "    ## Reduz 1g da memória para o sistema\n",
    "    memory = f'{mem_gib}g'\n",
    "\n",
    "    ## retorna a memória total do sistema\n",
    "    return memory\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def get_cores():\n",
    "    \"\"\"\n",
    "    Retorna a quantidade de cores (cpu) do sistema (reduzindo 1core para controle)\n",
    "    ----------\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    cores : int\n",
    "        CPU Cores total disponível\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variável local\n",
    "    cpu = 1\n",
    "\n",
    "    ## Recupera a quantidade de cores do sistema\n",
    "    cpus = psutil.cpu_count()\n",
    "    ## Reduz 1 core da capacidade total do sistema\n",
    "    cpu = round(int(cpus) * 0.7)\n",
    "\n",
    "    ## Verifica o tamanho mínimo e ajusta se for o caso\n",
    "    if cpu <= 0: cpu = 1\n",
    "\n",
    "    ## retorna a memória total do sistema\n",
    "    return cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração Spark e instância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_spark(name:str=None):\n",
    "    \"\"\"\n",
    "    Função para iniciar a instância do contexto spark\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    name : str, optional, default = DATE_TIME.strftime('%Y-%m%d') \n",
    "        Nome da estrutura de carga do yarn - spark\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    spark : sparkSession\n",
    "        Sessão spark para execução das instruções spark no contexto\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variáveis global\n",
    "    global __SPARK\n",
    "\n",
    "    ## Variáveis local\n",
    "    sc = None\n",
    "    \n",
    "    ## Verifica se a sessão spark já está ativa e válida\n",
    "    if __SPARK: return __SPARK\n",
    "\n",
    "    ## Verifica a instalação da jvm para o spark\n",
    "    findspark.init()\n",
    "\n",
    "    ### Cria a variável de ambiente\n",
    "    environment = ['PYSPARK_PYTHON', 'PYSPARK_DRIVER_PYTHON']\n",
    "    for var in environment:\n",
    "        os.environ[var] = sys.executable\n",
    "\n",
    "    ## Define o nome do contexto\n",
    "    if not name: name = DATE_TIME.strftime('%Y-%m%d') \n",
    "    name = f'process_{name}'\n",
    "\n",
    "    ## Verifica se o diretório destino existe, caso contrário cria o caminho\n",
    "    folder_create(folder=SPARK_PATH)\n",
    "\n",
    "    ## Configura o processo spark\n",
    "    conf = SparkConf() \\\n",
    "        .setAppName(name) \\\n",
    "        .setMaster('local[*]') \\\n",
    "        .setAll([\n",
    "            (\"spark.submit.deployMode\", \"client\"),\n",
    "            (\"spark.local.dir\", SPARK_PATH),\n",
    "            (\"spark.network.timeout\", \"39000\"),\n",
    "            (\"spark.executor.memory\", get_memory()), \n",
    "            (\"spark.executor.cores\", get_cores()), \n",
    "            (\"spark.driver.cores\", get_cores()), \n",
    "            (\"spark.driver.memoryOverhead\", \"2048\"),\n",
    "            (\"spark.memory.offHeap.enabled\", \"true\"),\n",
    "            (\"spark.memory.offHeap.size\", get_memory()),\n",
    "            (\"spark.dynamicAllocation.enabled\", \"false\"),\n",
    "            (\"spark.default.parallelism\", \"50\"),    \n",
    "            (\"spark.driver.maxResultSize\", \"0\"), \n",
    "            (\"spark.driver.supervise\", \"true\"),\n",
    "            (\"spark.sql.debug.maxToStringFields\", 2000),\n",
    "            (\"spark.sql.caseSensitive\", \"false\"),\n",
    "            (\"spark.driver.userClassPathFirst\", \"false\"),\n",
    "            (\"spark.standalone.submit.waitAppCompletion\", 'true'),\n",
    "            (\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\"),\n",
    "            (\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\"),\n",
    "            (\"spark.sql.parquet.mergeSchema\", \"false\"),\n",
    "            (\"spark.sql.parquet.filterPushdown\", 'false'),\n",
    "            (\"spark.sql.parquet.enableVectorizedReader\", \"false\"),\n",
    "            (\"spark.jars\", f\" \\\n",
    "                {PATH_APPLICATION}/drivers/gcs-connector-hadoop3-latest.jar, \\\n",
    "                {PATH_APPLICATION}/drivers/bigquery-connector-hadoop3-latest.jar, \\\n",
    "                {PATH_APPLICATION}/drivers/spark-bigquery-with-dependencies_2.12-0.22.0.jar, \\\n",
    "                {PATH_APPLICATION}/drivers/spark-bigquery-latest_2.12.jar, \\\n",
    "                {PATH_APPLICATION}/drivers/ngdbc-latest.jar, \\\n",
    "                {PATH_APPLICATION}/drivers/jetty-util-11.0.5.jar\"\n",
    "            ),\n",
    "            (\"spark.sql.execution.arrow.enabled\", \"true\"),\n",
    "            (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "            (\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\"),\n",
    "            (\"spark.history.fs.cleaner.enabled\", \"true\"),\n",
    "            (\"spark.eventLog.enabled\", \"false\"),\n",
    "            (\"spark.eventLog.overwrite\", \"true\"),\n",
    "            (\"spark.logConf\", \"false\")\n",
    "        ])\n",
    "\n",
    "    ## Cria o contexto spark\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "    ## Ajusta o log de erro\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    ## drivers para leitura de arquivos em google cloud\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "    sc._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", f\"{PATH_APPLICATION}/keys/{PROJECT_NAME}_key_google.json\")\n",
    "    \n",
    "    ## Cria o sessão spark\n",
    "    __SPARK = SparkSession.builder.config(conf=sc.getConf()).getOrCreate()\n",
    "\n",
    "    ## Retorna a sessão spark\n",
    "    return __SPARK\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def stop_spark():\n",
    "    \"\"\"\n",
    "    Encerra a operação com spark e limpa os temporários\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    spark : SparkSession, default = None\n",
    "        Sessão spark existente\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variáveis global\n",
    "    global __SPARK\n",
    "\n",
    "    try:\n",
    "        ## Encerra a sessao\n",
    "        __SPARK.stop()\n",
    "    finally:\n",
    "        ## Limpa o GC\n",
    "        gc.collect()\n",
    "\n",
    "    ## Limpa as pastas temporárias\n",
    "    folder_delete(SPARK_PATH)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def read_sql(project:str, dataset:str, sql:str):\n",
    "    \"\"\"\n",
    "    Carrega os dados via instrução sql via operação spark\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Sessão spark existente\n",
    "    project : str\n",
    "        Nome do Project_id de hospedagem no bigquery [project-XXXXXX]\n",
    "    dataset : str\n",
    "        Nome do Dataset_id de hospedagem no bigquery [operation]\n",
    "    sql : str\n",
    "        Instrução sql para execução no banco de dados\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    data : spark rdd (dataframe)\n",
    "        Retorno do dataframe contendo os dados da extração\n",
    "    total : int\n",
    "        Total de registros encontrados no dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variáveis local\n",
    "    data  = None\n",
    "    total = -1\n",
    "\n",
    "    ## Verifica se a sessão está aberta\n",
    "    spark = start_spark()\n",
    "\n",
    "    ## Captação dos dados de uma tabela\n",
    "    data = spark.read \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option('viewsEnabled', 'true') \\\n",
    "        .option('materializationProject', project) \\\n",
    "        .option('materializationDataset', dataset) \\\n",
    "        .option('optimizedEmptyProjection', 'true') \\\n",
    "        .option('type', 'direct') \\\n",
    "        .load(sql)\n",
    "\n",
    "    ## Organiza e padroniza os nomes das colunas\n",
    "    data = data.toDF(*[col.upper() for col in data.columns])\n",
    "    ## Contabiliza o total de registros do pacote\n",
    "    total = data.count()\n",
    "\n",
    "    ## Retorna o total do arquivo\n",
    "    return data, total\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def read_table(project:str, table:str, columns=None):\n",
    "    \"\"\"\n",
    "    Carrega os dados de uma tabela do bigquery\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    project : str\n",
    "        Nome do Project_id de hospedagem no bigquery [project-XXXXXX]\n",
    "    table : str\n",
    "        Nome da tabela de hospedagem no bigquery [files]\n",
    "    columns : list, optional, default = None\n",
    "        Sequência e nome das colunas do arquivo para subscrição ['ID', 'TIPO', 'NOME', 'DESCRICAO']\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    data : spark rdd (dataframe)\n",
    "        Retorno do dataframe contendo os dados da extração\n",
    "    total : int\n",
    "        Total de registros encontrados no dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variáveis local\n",
    "    data  = None\n",
    "    total = -1\n",
    "\n",
    "    ## Verifica se a sessão está aberta\n",
    "    spark = start_spark()\n",
    "\n",
    "    ## Captação dos dados de uma tabela\n",
    "    data = spark.read \\\n",
    "        .format('bigquery') \\\n",
    "        .option(\"parentProject\", project) \\\n",
    "        .option('project', project) \\\n",
    "        .option('table', table) \\\n",
    "        .option('viewsEnabled', 'true') \\\n",
    "        .option('optimizedEmptyProjection', 'false') \\\n",
    "        .load()\n",
    "\n",
    "    ## Organiza e padroniza os nomes das colunas\n",
    "    data = data.toDF(*columns) if columns else data.toDF(*[col.upper() for col in data.columns])\n",
    "    ## Contabiliza o total de registros do pacote\n",
    "    total = data.count()\n",
    "\n",
    "    ## Retorna o total do arquivo\n",
    "    return data, total\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def read_data(file:object, columns:str=None, header:bool=True, sep:str=None, encoding:str='utf-8', quote:str=None):\n",
    "    \"\"\"\n",
    "    Carrega os dados de um AQUIVO CSV OU PARQUET\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    file : object\n",
    "        Caminho + arquivo para leitura do arquivo ['/opt/data//programs_2021-03-18.parquet' or gs://[bucket_name]/landing/programs_2021-03-18.parquet]\n",
    "    columns : list, optional, default = None\n",
    "        Sequência e nome das colunas do arquivo para subscrição ['ID', 'TIPO', 'NOME', 'DESCRICAO']\n",
    "    header : bool, optional, default = True\n",
    "        Se o arquivo é lido com cabeçalho ou não [True or False] \n",
    "    sep : str, optional, default = None\n",
    "        Tipo do separador do arquivo [',', ';', '\\t', '|']\n",
    "    encoding : str, optional, default = 'utf-8'\n",
    "        Código de codificação da leitura do arquivo ['utf-8', 'utf-16', 'iso-8859-1']\n",
    "    quote : str, optional, default = \"\n",
    "        Caracter para escapar de linhas com erro\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    data : spark rdd (dataframe)\n",
    "        Retorno do dataframe contendo os dados da extração\n",
    "    total : int\n",
    "        Total de registros encontrados no dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variáveis local\n",
    "    total     = -1\n",
    "    data      = None\n",
    "    extension = file_extension(file=file)\n",
    "\n",
    "    ## Verifica se a sessão está aberta\n",
    "    spark = start_spark()\n",
    "\n",
    "    ## Verifica se a sessão está aberta e ativa no Spark\n",
    "    if not spark:\n",
    "        raise Exception('Sessão com o Spark não iniciada.')\n",
    "\n",
    "    ## O tipo do arquivo\n",
    "    if extension in ('.parquet'):\n",
    "        ## Realiza a leitura do arquivo parquet para validar a quantidade\n",
    "        data = spark.read \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .parquet(file)\n",
    "    elif extension in ('.json'):\n",
    "        data = spark.read \\\n",
    "            .option('multiline','true') \\\n",
    "            .json(file)\n",
    "    elif extension in ('.xlsx', '.xls', '.xlsm'):\n",
    "        ## Leitira do arquivo excel via pandas\n",
    "        data = pd.read_excel(\n",
    "            file, \n",
    "            engine='openpyxl'\n",
    "        )\n",
    "    elif extension in ('.csv', '.txt'):\n",
    "        ## Ajusta o caracter de quote\n",
    "        if quote == '': quote = None\n",
    "\n",
    "        ## Realiza a leitura dos dados e coloca em disco\n",
    "        data = spark.read.csv(\n",
    "            path=file,\n",
    "            header=header,\n",
    "            sep=sep,\n",
    "            encoding=encoding,\n",
    "            inferSchema=False,\n",
    "            ignoreLeadingWhiteSpace=True,\n",
    "            ignoreTrailingWhiteSpace=True,\n",
    "            multiLine=True,\n",
    "            charToEscapeQuoteEscaping=quote,\n",
    "            quote=quote,\n",
    "            escape=quote,\n",
    "            mode='FAILFAST',\n",
    "            unescapedQuoteHandling='RAISE_ERROR',\n",
    "        )\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "    ## Converte o pyspark.pandas dataframe para RDD\n",
    "    data = to_rdd(\n",
    "        data=data\n",
    "    )\n",
    "    ## Carrega o nome das colunas ou converte para maiúsculo caso exista\n",
    "    data = data.toDF(*columns) if columns else data.toDF(*[col.upper() for col in data.columns])\n",
    "    ## Total de registros do dataframe\n",
    "    total = data.count()\n",
    "\n",
    "    ## Retorna o total do arquivo\n",
    "    return data, total\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def save_data(data, bucket:str, table:str, mode:bool):\n",
    "    \"\"\"\n",
    "    Salva os dados de um rdd (dataframe) em uma tabela no bigquery\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    data : pandas.core.frame.DataFrame ou pyspark.sql.dataframe.DataFrame\n",
    "        Dataframe em memória / disco\n",
    "    bucket : str\n",
    "        Nome do bucket de destino dos dados [project-XXXXXX]\n",
    "    table : str\n",
    "        Nome da tabela de destino dos dados [project-XXXXXX.dataset.table]\n",
    "    mode : bool\n",
    "        Módulo para append = false ou overwrite = true\n",
    "    \"\"\"\n",
    "\n",
    "    ## Verifica se a sessão está aberta\n",
    "    spark = start_spark()\n",
    "\n",
    "    try:\n",
    "        ## Verifica se há a necessidade de conversão do dataframe para rdd\n",
    "        data = to_rdd(\n",
    "            data=data\n",
    "        )\n",
    "            \n",
    "        ## Monta a variável para adição ou subescrição\n",
    "        mode_type = 'overwrite' if mode else 'append'\n",
    "\n",
    "        ## Salva o dataframe em parquet local\n",
    "        data.write \\\n",
    "            .format('bigquery') \\\n",
    "            .option(\"temporaryGcsBucket\", BUCKET_NAME) \\\n",
    "            .option('table', table) \\\n",
    "            .mode(mode_type) \\\n",
    "            .save()\n",
    "    except Exception as ex:\n",
    "        raise Exception(f'DATABASE: Falha na gravação {bucket}.{table} no (MODULO: save_data) - {ex}')       \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def save_file(data, file:str, mode:bool=True):\n",
    "    \"\"\"\n",
    "    Salva os dados de um rdd (dataframe) em um arquivo local\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    data : pandas.core.frame.DataFrame ou pyspark.sql.dataframe.DataFrame\n",
    "        Dataframe (rdd) em memória / disco\n",
    "    file : str\n",
    "        Nome do arquivo para armazenamento em disco [/opt/data/programs_2021-03-18.csv]\n",
    "    mode : bool, optional, default = True\n",
    "        Módulo para append = false ou overwrite = true\n",
    "    \"\"\"\n",
    "\n",
    "    ## Verifica se a sessão está aberta\n",
    "    spark = start_spark()\n",
    "\n",
    "    ## Valida se o arquivo foi criado corretamente\n",
    "    try:\n",
    "        ## Verifica se o dataframe é do tipo pandas ou rdd\n",
    "        data = to_rdd(\n",
    "            data=data\n",
    "        )\n",
    "\n",
    "        ## Variáveis local\n",
    "        folder_base, extension = file_split(file=file)\n",
    "        mode_type = 'overwrite' if mode else 'append'\n",
    "\n",
    "        ## Verifica se a exportação é no formato excel\n",
    "        if extension.lower() in ('xlsx', 'xls', 'excel'):\n",
    "            extension = 'com.crealytics.spark.excel'\n",
    "\n",
    "        ## Criando pasta de destino caso não exista\n",
    "        folder_create(folder=folder_base)\n",
    "        \n",
    "        ## Salva o dataframe em parquet local\n",
    "        data.repartition(1) \\\n",
    "            .write.format(extension) \\\n",
    "            .mode(mode_type) \\\n",
    "            .save(\n",
    "                folder_base, \n",
    "                header=True,\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "\n",
    "        ## Verifica se o arquivo foi criado e ajusta o nome final\n",
    "        for name in path.Path(folder_base).glob('_SUCCESS'):\n",
    "\n",
    "            ## Varre o diretório criado \"parquet\" para validar os arquivos gerados\n",
    "            for item in glob.glob(f'{folder_base}/part-*.{extension}'):\n",
    "                ## Renomei o arquivo criado parquet part para o nome de exportação\n",
    "                os.rename(item, file)\n",
    "                \n",
    "                ## Verifica se o arquivo foi renomeado corretamente\n",
    "                if glob.glob(file):\n",
    "                    ## Apaga \n",
    "                    folder_delete(folder=folder_base)\n",
    "                    ## Remove o dataframe da memória\n",
    "                    data.unpersist(True)\n",
    "\n",
    "    except Exception as ex:\n",
    "        raise Exception(f'DATABASE: Falha na gravação do arquivo {file} no (MODULO: save_file) - {ex}')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "def to_rdd(data:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Carrega os dados de um pandas dataframe para um spark dataframe\n",
    "    ----------\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    data : pd.Dataframe\n",
    "        Dados de um pandas dataframe\n",
    "\n",
    "    Retornos\n",
    "    ----------\n",
    "    df : spark dataframe\n",
    "        Retorno do dataframe contendo os dados da extração\n",
    "    \"\"\"\n",
    "\n",
    "    ## Variáveis local\n",
    "    df = None \n",
    "\n",
    "    ## Verifica se a sessão está aberta\n",
    "    spark = start_spark()\n",
    "\n",
    "    if type(data) == pd.DataFrame:\n",
    "        ## Ajusta os tipos de colunas para string\n",
    "        data = data.astype('str')\n",
    "        ## Converte o pandas dataframe para spark RDD\n",
    "        df = spark.createDataFrame(data=data)\n",
    "    elif type(data) == ps.DataFrame:\n",
    "        ## Ajusta os tipos de colunas para string\n",
    "        data = data.astype('str')\n",
    "        ## Converte de pyspark.pandas para spark RDD\n",
    "        df = data.to_spark()\n",
    "    else:\n",
    "        ## Atualiza a variável local com o conteúdo \n",
    "        df = data\n",
    "\n",
    "    ## Retorna o spark datafrem\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplos de execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo um arquivo parquet de dentro do storage da Google em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Caminho do arquivo\n",
    "file = 'gs://bucket_name/staging/file_2022-02-19/*.parquet'\n",
    "### Leitura do arquivo no storage e retorno do dataframe e total de registros\n",
    "data, total = read_data(file=file)\n",
    "\n",
    "### Apresentação do resultado\n",
    "display(f'Total de registros lidos: {total}', 'Dados:', data.show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo uma tabela do BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lendo a tabela do BigQuery completa e retorno do dataframe e total de registros\n",
    "data, total = read_table(project=[project_XXXXX], table=[dataset.tabela]])\n",
    "\n",
    "### Apresentação do resultado\n",
    "display(f'Total de registros lidos: {total}', 'Dados:', data.show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo um sql no BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instrução sql\n",
    "sql = 'SELECT DISTINCT ID,GENDER FROM `project-XXXXX.dataset.table` WHERE ID > 0'\n",
    "\n",
    "## Captação dos dados de uma tabela\n",
    "data, total = read_sql(project=[project_XXXXX], dataset=[dataset_name], sql=sql)\n",
    "\n",
    "### Apresentação do resultado\n",
    "display(f'Total de registros lidos: {total}', 'Dados:', data.show(5, False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
